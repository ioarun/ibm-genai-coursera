{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Causal Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to build and train a decoder GPT-like model, which is great for generating text and other natural language processing tasks. We'll start with the basics, like getting our environment ready and preparing our data by breaking it down into tokens and turning those tokens into numbers the model can understand. Then, we'll dive into building the model itself, focusing on how it learns to pay attention to different parts of the text to generate text. Along the way, we'll cover how to train our model with data. Finally, we'll see how to use our trained model to create text based on what it has learned.\n",
    "\n",
    "## GPT models\n",
    "\n",
    "GPT (Generative Pretrained Transformer) is a decoder-only model because it is trained using a causal language modeling objective, where the goal is to predict the next token in a sequence given the previous tokens. During training, the input sequence is shifted to the right, and the model learns to generate output tokens autoregressively, one at a time. This process allows GPT to generate coherent and contextually relevant text based on the given input prompt. In this lab you will learn how to create and train a decoder-only GPT-like model. However, please note that the actual GPT models are larger models and are trained on massive training data for specific NLP tasks.\n",
    "\n",
    "## GPT vs. ChatGPT\n",
    "\n",
    "GPT and ChatGPT are both AI models developed by OpenAI, but they serve different purposes and have distinct functionalities.\n",
    "\n",
    "GPT is a family of large-scale transformer-based language models trained on diverse internet text data. GPT models are designed for a wide range of natural language processing tasks, such as text generation, translation, summarization, and question-answering. They generate responses based on the input text (prompt) but do not maintain a consistent conversation history.\n",
    "\n",
    "On the other hand, ChatGPT is a fine-tuned version of the GPT model, specifically designed for conversational AI applications. It is trained to maintain a consistent conversation history and generate contextually relevant responses, making it more suitable for chatbot-like interactions. ChatGPT excels at understanding and generating human-like dialogues, providing coherent and engaging responses in a conversational setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Text-pipeline\">Text pipeline</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Dataset\">Dataset</a></li>\n",
    "            <li> <a href=\"#Collate-function\">Collate function</a></li> \n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Model-prerequisites\">Model prerequisites</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Masking\">Masking</a></li>\n",
    "            <li><a href=\"#Positional-encoding\">Positional encoding</a></li>\n",
    "            <li><a href=\"#Token-embedding\">Token embedding</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Custom-GPT-model-architecture\">Custom GPT model architecture</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Model-configuration-and-initialization\">Model configuration and initialization</a></li>\n",
    "            <li><a href=\"#Iterating-through-data-samples\">Iterating through data samples</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Full-output\">Full output</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Decoding-the-differences:-Training-vs.-inference\">Decoding the differences: Training vs. inference</a></li>\n",
    "            <li><a href=\"#Training-the-model\">Training the model</a></li>\n",
    "            <li><a href=\"#Loading-the-saved-model\">Loading the saved model</a></li>\n",
    "            <li><a href=\"#Loading-GPT2-model-from-HuggingFace\">Loading GPT2 model from HuggingFace</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "      <li>\n",
    "        <a href=\"#Exercise:-Creating-a-decoder-model\">Exercise: Creating a decoder model</a>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After going through this tutorial, you'll be able to:\n",
    "\n",
    "- Understand how to pick random samples from your data and break text down into tokens.\n",
    "- Learn how to turn tokens into a vocabulary that your model can use to understand and process text.\n",
    "- Master the setup of the decoder model architecture, including how it uses attention to generate text.\n",
    "- Get familiar with training a decoder model, including how to feed it data and improve its performance over time.\n",
    "- Use your trained decoder model to generate text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# %pip install numpy==1.26.0 portalocker==2.8.2 pandas==2.2.1 matplotlib==3.9.0 scikit-learn==1.5.0 transformers==4.35.2\n",
    "# %pip install torch==2.3.0+cpu torchdata==0.9.0+cpu torchtext==0.18.0+cpu torchvision torchaudio\\\n",
    "#     --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torchdata**: Enhances data loading and preprocessing functionalities for PyTorch, streamlining the workflow for machine learning models.\n",
    "- **portalocker**: Provides a mechanism to lock files, ensuring that only one process can access a file at a time, useful for managing file resources in concurrent applications.\n",
    "- **torchtext**: Offers utilities for text processing and datasets in PyTorch, simplifying the preparation of data for NLP tasks.\n",
    "- **matplotlib**: A plotting library for creating static, interactive, and animated visualizations in Python, commonly used for data visualization and graphical plotting tasks.\n",
    "\n",
    "Each of these libraries is used to handle different aspects of data preparation, processing, and model training for machine learning and natural language processing applications, enhancing the overall workflow and capabilities of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import IMDB,PennTreebank\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pipeline\n",
    "## Dataset\n",
    "The code loads the IMDB dataset into training, and validation sets. It then creates an iterator for the training set and loops through the first 10 samples, printing each one. This process simulates how one might manually iterate over a dataset without using PyTorch's `DataLoader` for batch processing and data management.\n",
    "\n",
    "When training language models, it is generally advisable to use general-domain text. However, in this case, we are using the IMDB dataset, which is well-suited for classification tasks. However, we use IMDB due to its smaller size and compatibility with machines that have limited RAM. For language modeling tasks, some datasets you can consider include: [PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#penntreebank), [WikiText-2](https://pytorch.org/text/0.8.1/datasets.html#wikitext-2), [WikiText103](https://pytorch.org/text/0.8.1/datasets.html#wikitext103)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_iter, val_iter = IMDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an iterator for the train data loader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_itr=iter(train_iter)\n",
    "# retrieving the third first record\n",
    "next(data_itr)\n",
    "next(data_itr)\n",
    "next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our device (CPU or GPU) for training. We'll check if a GPU is available and use it; otherwise, we'll use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing data\n",
    "The provided code is used for preprocessing text data, particularly for NLP tasks, with a focus on tokenization and vocabulary building.\n",
    "\n",
    "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`). These tokens are used for unknown words, padding, and end of sentence respectively.\n",
    "    - `UNK_IDX`: Index for unknown words.\n",
    "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
    "    - `EOS_IDX`: Index representing the end of a sentence (though not explicitly used here as the EOS symbol is set to an empty string).\n",
    "\n",
    "- **`yield_tokens` Function**: A generator function that iterates through a dataset (`data_iter`), tokenizing each data sample using a `tokenizer` function, and yields one tokenized sample at a time.\n",
    "\n",
    "- **Vocabulary building**: Constructs a vocabulary from the tokenized dataset. The `build_vocab_from_iterator` function processes tokens generated by `yield_tokens`, includes special tokens (`special_symbols`) at the beginning of the vocabulary, and sets a minimum frequency (`min_freq=1`) for tokens to be included.\n",
    "\n",
    "- **Default index for unknown tokens**: Sets a default index for tokens not found in the vocabulary (`UNK_IDX`), ensuring that out-of-vocabulary words are handled as unknown tokens.\n",
    "\n",
    "- **`text_to_index` function**: Converts a given text into a sequence of indices based on the built vocabulary. This function is essential for transforming raw text into a numerical format that can be processed by machine learning models.\n",
    "\n",
    "- **`index_to_en` function**: Transforms a sequence of indices back into a readable string. It's useful for interpreting the outputs of models and converting numerical predictions back into text.\n",
    "\n",
    "- **Check functionality**: Demonstrates the use of `index_to_en` by converting a tensor of indices `[0,1,2]` back into their corresponding special symbols. This helps verify that the vocabulary and index conversion functions are working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,data_sample in data_iter:\n",
    "        yield  tokenizer(data_sample)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
    "vocab.set_default_index(UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note : **The above block should be complete in less than 20 seconds.If it takes longer than that, it is advisable to `Restart the kernel` and run the cells following the cell containing the `pip install` commands to ensure that the function mentioned above operates as intended.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text to index and index to Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <pad> <|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "index_to_en(torch.tensor([0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate function\n",
    "In the context of our decoder model, we aim to create a collate function. This function takes a block of text as input and produces a modified block of text as output. The actual text transformation is achieved through the use of the `get_sample(block_size, text)` function. The **get_sample** function generates a random text sample(src_sequence) and its subsequent sequence(tgt_sequence) from a given text for language model training. It ensures the sample fits within the specified block size and adjusts for text shorter than the block size, returning both the source and target sequences for model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(block_size, text):\n",
    "    # Determine the length of the input text\n",
    "    sample_leg = len(text)\n",
    "    # Calculate the stopping point for randomly selecting a sample\n",
    "    # This ensures the selected sample doesn't exceed the text length\n",
    "    random_sample_stop = sample_leg - block_size\n",
    "\n",
    "\n",
    "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
    "    if random_sample_stop >= 1:\n",
    "        # Randomly select a starting point for the sample\n",
    "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
    "        # Define the endpoint of the sample\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Create the input and target sequences\n",
    "        src_sequence = text[random_start:stop]\n",
    "        tgt_sequence= text[random_start + 1:stop + 1]\n",
    "\n",
    "    # Handle the case where the text length is exactly equal or less the block size\n",
    "    elif random_sample_stop <= 0:\n",
    "        # Start from the beginning and use the entire text\n",
    "        random_start = 0\n",
    "        stop = sample_leg\n",
    "        src_sequence= text[random_start:stop]\n",
    "        tgt_sequence = text[random_start + 1:stop]\n",
    "        # Append an empty string to maintain sequence alignment\n",
    "        tgt_sequence.append( '<|endoftext|>')\n",
    "\n",
    "    return src_sequence, tgt_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test `get_sample(block_size, text)` first and get batch of texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "\n",
    "batch_of_tokens=[]\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  _,text =next(iter(train_iter))\n",
    "  batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first smaple of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'rented',\n",
       "  'i',\n",
       "  'am',\n",
       "  'curious-yellow',\n",
       "  'from',\n",
       "  'my',\n",
       "  'video',\n",
       "  'store',\n",
       "  'because',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'controversy',\n",
       "  'that',\n",
       "  'surrounded',\n",
       "  'it',\n",
       "  'when',\n",
       "  'it',\n",
       "  'was',\n",
       "  'first',\n",
       "  'released',\n",
       "  'in',\n",
       "  '1967',\n",
       "  '.',\n",
       "  'i',\n",
       "  'also',\n",
       "  'heard',\n",
       "  'that',\n",
       "  'at',\n",
       "  'first',\n",
       "  'it',\n",
       "  'was',\n",
       "  'seized',\n",
       "  'by',\n",
       "  'u',\n",
       "  '.',\n",
       "  's',\n",
       "  '.',\n",
       "  'customs',\n",
       "  'if',\n",
       "  'it',\n",
       "  'ever',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'enter',\n",
       "  'this',\n",
       "  'country',\n",
       "  ',',\n",
       "  'therefore',\n",
       "  'being',\n",
       "  'a',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'films',\n",
       "  'considered',\n",
       "  'controversial',\n",
       "  'i',\n",
       "  'really',\n",
       "  'had',\n",
       "  'to',\n",
       "  'see',\n",
       "  'this',\n",
       "  'for',\n",
       "  'myself',\n",
       "  '.',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'is',\n",
       "  'centered',\n",
       "  'around',\n",
       "  'a',\n",
       "  'young',\n",
       "  'swedish',\n",
       "  'drama',\n",
       "  'student',\n",
       "  'named',\n",
       "  'lena',\n",
       "  'who',\n",
       "  'wants',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'everything',\n",
       "  'she',\n",
       "  'can',\n",
       "  'about',\n",
       "  'life',\n",
       "  '.',\n",
       "  'in',\n",
       "  'particular',\n",
       "  'she',\n",
       "  'wants',\n",
       "  'to',\n",
       "  'focus',\n",
       "  'her',\n",
       "  'attentions',\n",
       "  'to',\n",
       "  'making',\n",
       "  'some',\n",
       "  'sort',\n",
       "  'of',\n",
       "  'documentary',\n",
       "  'on',\n",
       "  'what',\n",
       "  'the',\n",
       "  'average',\n",
       "  'swede',\n",
       "  'thought',\n",
       "  'about',\n",
       "  'certain',\n",
       "  'political',\n",
       "  'issues',\n",
       "  'such',\n",
       "  'as',\n",
       "  'the',\n",
       "  'vietnam',\n",
       "  'war',\n",
       "  'and',\n",
       "  'race',\n",
       "  'issues',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  '.',\n",
       "  'in',\n",
       "  'between',\n",
       "  'asking',\n",
       "  'politicians',\n",
       "  'and',\n",
       "  'ordinary',\n",
       "  'denizens',\n",
       "  'of',\n",
       "  'stockholm',\n",
       "  'about',\n",
       "  'their',\n",
       "  'opinions',\n",
       "  'on',\n",
       "  'politics',\n",
       "  ',',\n",
       "  'she',\n",
       "  'has',\n",
       "  'sex',\n",
       "  'with',\n",
       "  'her',\n",
       "  'drama',\n",
       "  'teacher',\n",
       "  ',',\n",
       "  'classmates',\n",
       "  ',',\n",
       "  'and',\n",
       "  'married',\n",
       "  'men',\n",
       "  '.',\n",
       "  'what',\n",
       "  'kills',\n",
       "  'me',\n",
       "  'about',\n",
       "  'i',\n",
       "  'am',\n",
       "  'curious-yellow',\n",
       "  'is',\n",
       "  'that',\n",
       "  '40',\n",
       "  'years',\n",
       "  'ago',\n",
       "  ',',\n",
       "  'this',\n",
       "  'was',\n",
       "  'considered',\n",
       "  'pornographic',\n",
       "  '.',\n",
       "  'really',\n",
       "  ',',\n",
       "  'the',\n",
       "  'sex',\n",
       "  'and',\n",
       "  'nudity',\n",
       "  'scenes',\n",
       "  'are',\n",
       "  'few',\n",
       "  'and',\n",
       "  'far',\n",
       "  'between',\n",
       "  ',',\n",
       "  'even',\n",
       "  'then',\n",
       "  'it',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'not',\n",
       "  'shot',\n",
       "  'like',\n",
       "  'some',\n",
       "  'cheaply',\n",
       "  'made',\n",
       "  'porno',\n",
       "  '.',\n",
       "  'while',\n",
       "  'my',\n",
       "  'countrymen',\n",
       "  'mind',\n",
       "  'find',\n",
       "  'it',\n",
       "  'shocking',\n",
       "  ',',\n",
       "  'in',\n",
       "  'reality',\n",
       "  'sex',\n",
       "  'and',\n",
       "  'nudity',\n",
       "  'are',\n",
       "  'a',\n",
       "  'major',\n",
       "  'staple',\n",
       "  'in',\n",
       "  'swedish',\n",
       "  'cinema',\n",
       "  '.',\n",
       "  'even',\n",
       "  'ingmar',\n",
       "  'bergman',\n",
       "  ',',\n",
       "  'arguably',\n",
       "  'their',\n",
       "  'answer',\n",
       "  'to',\n",
       "  'good',\n",
       "  'old',\n",
       "  'boy',\n",
       "  'john',\n",
       "  'ford',\n",
       "  ',',\n",
       "  'had',\n",
       "  'sex',\n",
       "  'scenes',\n",
       "  'in',\n",
       "  'his',\n",
       "  'films',\n",
       "  '.',\n",
       "  'i',\n",
       "  'do',\n",
       "  'commend',\n",
       "  'the',\n",
       "  'filmmakers',\n",
       "  'for',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'any',\n",
       "  'sex',\n",
       "  'shown',\n",
       "  'in',\n",
       "  'the',\n",
       "  'film',\n",
       "  'is',\n",
       "  'shown',\n",
       "  'for',\n",
       "  'artistic',\n",
       "  'purposes',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'just',\n",
       "  'to',\n",
       "  'shock',\n",
       "  'people',\n",
       "  'and',\n",
       "  'make',\n",
       "  'money',\n",
       "  'to',\n",
       "  'be',\n",
       "  'shown',\n",
       "  'in',\n",
       "  'pornographic',\n",
       "  'theaters',\n",
       "  'in',\n",
       "  'america',\n",
       "  '.',\n",
       "  'i',\n",
       "  'am',\n",
       "  'curious-yellow',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'film',\n",
       "  'for',\n",
       "  'anyone',\n",
       "  'wanting',\n",
       "  'to',\n",
       "  'study',\n",
       "  'the',\n",
       "  'meat',\n",
       "  'and',\n",
       "  'potatoes',\n",
       "  '(',\n",
       "  'no',\n",
       "  'pun',\n",
       "  'intended',\n",
       "  ')',\n",
       "  'of',\n",
       "  'swedish',\n",
       "  'cinema',\n",
       "  '.',\n",
       "  'but',\n",
       "  'really',\n",
       "  ',',\n",
       "  'this',\n",
       "  'film',\n",
       "  'doesn',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'have',\n",
       "  'much',\n",
       "  'of',\n",
       "  'a',\n",
       "  'plot',\n",
       "  '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=batch_of_tokens[0][0:100]\n",
    "text[0:100]\n",
    "batch_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the get_sample function with a block size of 100, where the output includes both the source sequence and the target sequence, with the target sequence being the source sequence shifted by one character, you can use the following code as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=10\n",
    "src_sequences, tgt_sequence=get_sample( block_size, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if seqence is shiffted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  [',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i']\n",
      "tgt:  ['therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i', 'really']\n"
     ]
    }
   ],
   "source": [
    "print(\"src: \",src_sequences)\n",
    "print(\"tgt: \",tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates batches of source (`src_batch`) and target (`tgt_batch`) sequences from a dataset for training NLP models. It loops through the dataset to extract text samples, generates corresponding source and target sequences using the `get_sample` function, converts them into vocabulary indices, and then into PyTorch tensors. Each iteration appends these sequences to their respective batch lists and prints their details, including text, indices, and tensor shapes, for two samples per batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Source Sequence (Text): ['country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial']\n",
      "Source Sequence (Indices): [800, 5, 1487, 118, 6, 372, 9, 129, 1206, 3535]\n",
      "Source Sequence (Shape): torch.Size([10])\n",
      "Target Sequence (Text): [',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'i']\n",
      "Target Sequence (Indices): [5, 1487, 118, 6, 372, 9, 129, 1206, 3535, 13]\n",
      "Target Sequence (Shape): torch.Size([10])\n",
      "Sample 1:\n",
      "Source Sequence (Text): ['her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.']\n",
      "Source Sequence (Indices): [57, 615, 1550, 5, 9546, 5, 7, 1188, 419, 3]\n",
      "Source Sequence (Shape): torch.Size([10])\n",
      "Target Sequence (Text): ['drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men', '.', 'what']\n",
      "Target Sequence (Indices): [615, 1550, 5, 9546, 5, 7, 1188, 419, 3, 54]\n",
      "Target Sequence (Shape): torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store source and target sequences\n",
    "src_batch, tgt_batch = [], []\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Loop to create batches of source and target sequences\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Retrieve the next data point from the training iterator\n",
    "    _,text = next(iter(train_iter))\n",
    "\n",
    "    # Generate source and target sequences using the get_sample function\n",
    "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
    "\n",
    "    # Convert source and target sequences to tokenized vocabulary indices\n",
    "    src_sequence_indices = vocab(src_sequence_text)\n",
    "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
    "\n",
    "    # Convert the sequences to PyTorch tensors with dtype int64\n",
    "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
    "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
    "\n",
    "    # Append the source and target sequences to their respective batches\n",
    "    src_batch.append(src_sequence)\n",
    "    tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    # Print the output for every 2nd sample (adjust as needed)\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
    "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
    "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
    "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
    "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
    "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collate_batch` function prepares batches of source and target sequences for training by processing each text sample in a given batch. It generates source and target sequences using the `get_sample` function with a specified block size, converts these sequences to indices using a vocabulary, and transforms them into PyTorch tensors. The sequences are then padded to ensure uniform length across the batch. Finally, it returns the padded source and target batches, ready for training on the specified device (`DEVICE`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=30\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for _,_textt in batch:\n",
    "      src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
    "      src_sequence=vocab(src_sequence)\n",
    "      tgt_sequence=vocab(tgt_sequence)\n",
    "      src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
    "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
    "      src_batch.append(src_sequence)\n",
    "      tgt_batch.append(tgt_sequence)\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "\n",
    "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up data loaders for the training, validation, and testing sets using the `DataLoader` class, with each set utilizing a custom `collate_batch` function for batch processing. The data loaders handle batches of size 1 for simplicity and shuffle the data for randomized access. After initializing the training data loader, it fetches the first batch of source (`src`) and target (`tgt`) sequences. It then iterates over each token in the source sequence, converts them back to text using the `index_to_en` function, and prints the resulting sentences, demonstrating how to access and display preprocessed data ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader= DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through data samples\n",
    "\n",
    "The provided code iterates through batches of source-target pairs from a data loader. It demonstrates how to access and print a few samples from the dataset:\n",
    "\n",
    "- We initialize an iterator over the data loader named `dataset`.\n",
    "- A loop runs for 10 iterations to fetch and print the first 10 source-target pairs. For each pair:\n",
    "    - `src` and `trt` (short for target) hold the batch of source and target sequences respectively.\n",
    "    - The `index_to_en` function is used to convert these sequences from numerical indices back to readable text.\n",
    "    - The `sample` number and corresponding source and target texts are printed out.\n",
    "\n",
    "After printing the first 10 samples, the code continues to iterate through the dataset:\n",
    "\n",
    "- It prints the shape of the target and source tensors for the next batch, which provides information about the number of tokens and batch size.\n",
    "- The `index_to_en` function is again used to convert the first sequence of the batch from indices to text for both source and target.\n",
    "- Only the first pair of the remaining batches is printed, and then the loop breaks.\n",
    "\n",
    "This process is useful for verifying that the data loader is functioning correctly and that the sequences are being properly transformed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0\n",
      "sorce: there ' s nudity , but hey , there ' s free porn on the internet for whomever likes it . and its just silly how they forced tits into\n",
      "\n",
      "\n",
      "target: ' s nudity , but hey , there ' s free porn on the internet for whomever likes it . and its just silly how they forced tits into every\n",
      "\n",
      "\n",
      "sample 1\n",
      "sorce: uhm , simplistic . the acting rose into good for a high school play territory from time to time . my feeling was this was filmed in a day --\n",
      "\n",
      "\n",
      "target: , simplistic . the acting rose into good for a high school play territory from time to time . my feeling was this was filmed in a day -- please\n",
      "\n",
      "\n",
      "sample 2\n",
      "sorce: . i could have looked past that if the story wasn ' t so lousy . if there was more of a background story , it would have been better\n",
      "\n",
      "\n",
      "target: i could have looked past that if the story wasn ' t so lousy . if there was more of a background story , it would have been better .\n",
      "\n",
      "\n",
      "sample 3\n",
      "sorce: in agony over having seen it . it ' s so bad , you have to wonder how anyone could write this tripe , much less allow it to be\n",
      "\n",
      "\n",
      "target: agony over having seen it . it ' s so bad , you have to wonder how anyone could write this tripe , much less allow it to be loose\n",
      "\n",
      "\n",
      "sample 4\n",
      "sorce: not prepared ! you may think you are some sort of tough guy by renting this but this movie will break you , push you to the ground and urinate\n",
      "\n",
      "\n",
      "target: prepared ! you may think you are some sort of tough guy by renting this but this movie will break you , push you to the ground and urinate on\n",
      "\n",
      "\n",
      "sample 5\n",
      "sorce: freaky guy had bested martial arts expert porn queens and a couple out doors type jocks he falls so easily to the frying pan of a skinny defenseless girl next\n",
      "\n",
      "\n",
      "target: guy had bested martial arts expert porn queens and a couple out doors type jocks he falls so easily to the frying pan of a skinny defenseless girl next door\n",
      "\n",
      "\n",
      "sample 6\n",
      "sorce: . i heard the dialog , saw the acting and all i could do was make faces . i also think that the dance movie theme is being overdone .\n",
      "\n",
      "\n",
      "target: i heard the dialog , saw the acting and all i could do was make faces . i also think that the dance movie theme is being overdone . at\n",
      "\n",
      "\n",
      "sample 7\n",
      "sorce: dying love interest her character was conceived as bold and free and touching and uninhibited and full of life even though dying , and was probably meant to be played\n",
      "\n",
      "\n",
      "target: love interest her character was conceived as bold and free and touching and uninhibited and full of life even though dying , and was probably meant to be played with\n",
      "\n",
      "\n",
      "sample 8\n",
      "sorce: our ' hubble ' ! nevertheless i think this a bad movie . film-technically it ' s a good one . nice shots and script , most good fitting music\n",
      "\n",
      "\n",
      "target: ' hubble ' ! nevertheless i think this a bad movie . film-technically it ' s a good one . nice shots and script , most good fitting music ,\n",
      "\n",
      "\n",
      "sample 9\n",
      "sorce: true facts of our space achievements in this film . but authors had in mind something else . . . ( we are big and beautiful country with intelligent people\n",
      "\n",
      "\n",
      "target: facts of our space achievements in this film . but authors had in mind something else . . . ( we are big and beautiful country with intelligent people living\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset=iter(dataloader)\n",
    "for sample in range(10):\n",
    "  src,trt=next(dataset)\n",
    "  print(\"sample\",sample)\n",
    "  print(\"sorce:\",index_to_en(src))\n",
    "  print(\"\\n\")\n",
    "  print(\"target:\",index_to_en(trt))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1])\n",
      "torch.Size([30, 1])\n",
      "script\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "for  src,trt in dataset:\n",
    "    print(trt.shape)\n",
    "    print(src.shape)\n",
    "    print(index_to_en(src[0,:]))\n",
    "    print(index_to_en(trt[0,:]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure source and target is shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: script , effects , score , and cinematography are what you would expect-- b-grade . some familiar faces are in here , and unless you are a mega fan of\n",
      "target: , effects , score , and cinematography are what you would expect-- b-grade . some familiar faces are in here , and unless you are a mega fan of colm\n"
     ]
    }
   ],
   "source": [
    "print(\"source:\",index_to_en(src))\n",
    "print(\"target:\",index_to_en(trt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've covered data preparation, let's move on to understanding the key components of the Transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "In transformers, masking is crucial for ensuring certain positions are not attended to. The function ```generate_square_subsequent_mask``` produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens of target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```create_mask function```, on the other hand, generates source masks, based on the provided source sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src,device=DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask,src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check an example source tensor and its associated masks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace first four tokens with PAD token so we can also check how pad tokens are masked using padding_mask\n",
    "src[0:4]=PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1],\n",
       "        [    1],\n",
       "        [    1],\n",
       "        [    1],\n",
       "        [  727],\n",
       "        [    5],\n",
       "        [    7],\n",
       "        [  731],\n",
       "        [   32],\n",
       "        [   54],\n",
       "        [   26],\n",
       "        [   63],\n",
       "        [46672],\n",
       "        [ 6520],\n",
       "        [    3],\n",
       "        [   55],\n",
       "        [ 1307],\n",
       "        [ 1555],\n",
       "        [   32],\n",
       "        [   14],\n",
       "        [  130],\n",
       "        [    5],\n",
       "        [    7],\n",
       "        [  588],\n",
       "        [   26],\n",
       "        [   32],\n",
       "        [    6],\n",
       "        [11265],\n",
       "        [  372],\n",
       "        [    9]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask,padding_mask = create_mask(src)\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "The Transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the embeddings of the tokens. These encodings have a fixed pattern based on their position in the sequence.\n",
    "\n",
    "GPT uses trainable positional encodings. Unlike fixed positional encodings (such as sinusoidal encodings used in the original Transformer paper), trainable positional encodings are learned during the model training process.\n",
    "\n",
    "Trainable positional encodings are implemented as a set of learnable parameters, one for each position in the input sequence. These parameters have the same dimensionality as the token embeddings. During training, the model updates the positional encoding parameters along with the other model parameters to capture the positional information more effectively.\n",
    "\n",
    "The use of trainable positional encodings in GPT allows the model to learn more flexible and task-specific positional representations, potentially improving its performance on various natural language processing tasks.\n",
    "\n",
    "In the context of this lab, we stick with fixed positional encoding for the sake of simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add positional information to the input tokens\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token embedding\n",
    "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
    "\n",
    "The `TokenEmbedding` class below converts numerical tokens into embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom GPT model architecture\n",
    "\n",
    "The `CustomGPTModel` class defines a transformer-based model architecture for generative pre-trained models. This model aims to generate text and perform various NLP tasks. Below is an explanation of the main components of the class:\n",
    "\n",
    "- **Initialization (`__init__`)**: The constructor takes several parameters including `embed_size`, `vocab_size`, `num_heads`, `num_layers`, `max_seq_len`, and `dropout`. It initializes the embedding layer, positional encoding, transformer encoder layers, and a linear layer (`lm_head`) for generating logits over the vocabulary.\n",
    "\n",
    "- **Weight initialization (`init_weights`)**: This method initializes the weights of the model for better training convergence. The Xavier uniform initialization is used, which is a common practice for initializing weights in deep learning.\n",
    "\n",
    "- **Decoder (`decoder`)**: Although named `decoder`, this method currently functions as the forward pass through the transformer encoder layers, followed by the generation of logits for the language modeling task. It handles the addition of positional encodings to the embeddings and applies a mask if necessary.\n",
    "\n",
    "- **Forward pass (`forward`)**: This method is similar to the `decoder` method and defines the forward computation of the model. It processes the input through embedding layers, positional encoding, transformer encoder layers, and produces the final output using the `lm_head`.\n",
    "\n",
    "- **Mask generation**: Both `decoder` and `forward` methods contain logic to generate a square causal mask if no source mask is provided. This mask ensures that the prediction for a position does not depend on the future tokens in the sequence, which is important for the autoregressive nature of GPT models.\n",
    "\n",
    "- **Commented out decoder**: A section of the code is commented out, suggesting an initial design where a transformer decoder layer was considered. However, the final implementation uses only encoder layers, which is a common simplification for models focusing on language modeling and generation.\n",
    "\n",
    "This class effectively encapsulates the necessary components to create a GPT-like model, allowing for training on language modeling tasks and text generation applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_weights()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
    "\n",
    "        print( embed_size )\n",
    "\n",
    "\n",
    "        # Remaining layers are part of the TransformerDecoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.embed_size = embed_size\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "      for p in self.parameters():\n",
    "          if p.dim() > 1:\n",
    "              nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_mask(src,device=DEVICE):\n",
    "        src_seq_len = src.shape[0]\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        return src_mask,src_padding_mask\n",
    "\n",
    "    def decoder(self, x,src_mask):\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        output = self.transformer_encoder(x, src_mask)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self,x,src_mask=None,key_padding_mask=None):\n",
    "\n",
    "        seq_length = x.size(0)\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask, src_padding_mask = create_mask(x)\n",
    "\n",
    "        output = self.transformer_encoder(x, src_mask,key_padding_mask)\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration and initialization\n",
    "\n",
    "Here, we configure and instantiate a Custom GPT Model with the following specifications:\n",
    "\n",
    "- `ntokens`: The total number of unique tokens in the vocabulary, which the model will use to represent words.\n",
    "- `emsize`: The size of each embedding vector. In this model, each word will be represented by a 200-dimensional vector.\n",
    "- `nlayers`: The number of transformer encoder layers in the model. We are using two layers in this configuration.\n",
    "- `nhead`: The number of attention heads in the multi-head attention mechanism. The model will use two attention heads.\n",
    "- `dropout`: A regularization technique where randomly selected neurons are ignored during training to prevent overfitting. Here, we set the dropout probability to 0.2.\n",
    "\n",
    "After setting these hyperparameters, we create an instance of `CustomGPTModel` by passing in the embedding size, number of attention heads, number of layers, vocabulary size, and dropout probability. The model is then moved to the specified `DEVICE`, which could be a CPU or GPU, for training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "\n",
    "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting \n",
    "In order to get the model to generate text (next token), you will need to create an starting point, which we call prompt, for the model to append tokens to it and generate text. Verify that the prompt is neither None nor too long, then proceed to tokenize it, convert it into indices, and reshape as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "    # Handle None prompt\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "\n",
    "    # Handle long prompts\n",
    "    if number_of_tokens > block_size:\n",
    "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
    "\n",
    "    prompt_indices = vocab(tokens)\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "    return prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some differnt exmaples where the input is None or longer than block size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sorry, prompt cannot be empty. Please enter a valid prompt:  this is the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the\n"
     ]
    }
   ],
   "source": [
    "print(index_to_en(encode_prompt(None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a prompt to get model generate next words .\n"
     ]
    }
   ],
   "source": [
    "print(index_to_en(encode_prompt(\"This is a prompt to get model generate next words.\" ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets encode a text prompt and run it through the decoder part of the model:\n",
    "\n",
    "- The `decoder` method of the `CustomGPTModel` instance `model` is called with the encoded prompt and without a source mask (`src_mask=None`), indicating that it will not mask any part of the sequence during processing. The decoder will handle creating a causal mask internally if required.\n",
    "- The output `logits` represents the model's raw predictions for each token position, which can be further processed (e.g., by applying a softmax function) to obtain the probabilities of the next token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   15],\n",
       "        [   11],\n",
       "        [    6],\n",
       "        [33700],\n",
       "        [   10],\n",
       "        [   86],\n",
       "        [ 2076],\n",
       "        [ 5673],\n",
       "        [  388],\n",
       "        [  665],\n",
       "        [    3]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_encoded=encode_prompt(\"This is a prompt to get model generate next words.\").to(DEVICE)\n",
    "prompt_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.decoder(prompt_encoded,src_mask=None).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11 tokens per output, an additional batch dimension, along with corresponding logits values for each word in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 68813])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape it such that the batch dimension becomes five\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 68813])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits.transpose(0, 1)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits contains logits for each token in the sequence generated by the decoder we just need the last one for the next word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 68813])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_preiction =logits[:,-1]\n",
    "logit_preiction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index of next word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37557], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _, next_word_index = torch.max(logit_preiction, dim=1)\n",
    " next_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8-year-olds'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_en(next_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive text generation\n",
    "In decoder models, we simply append the output to the input to generate the next response. We stop this process when we encounter the end-of-sequence tag <|endoftext|> or if the input becomes too large. We will implement it as a function later in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"this is the beginning of\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that the prompt is of the maximum input size and make a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device for prompt_encoded: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "print(\"Device for prompt_encoded:\", prompt_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Shape of logits at step 0: torch.Size([1, 5, 68813])\n",
      "Shape of logit_prediction at step 0: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 0: torch.Size([1, 1])\n",
      "Sequence for step 0: ['this', 'is', 'the', 'beginning', 'of', 'nondescript']\n",
      "Shape of prompt_encoded after concatenation at step 0: torch.Size([6, 1])\n",
      " \n",
      "Shape of logits at step 1: torch.Size([1, 6, 68813])\n",
      "Shape of logit_prediction at step 1: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 1: torch.Size([1, 1])\n",
      "Sequence for step 1: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y']\n",
      "Shape of prompt_encoded after concatenation at step 1: torch.Size([7, 1])\n",
      " \n",
      "Shape of logits at step 2: torch.Size([1, 7, 68813])\n",
      "Shape of logit_prediction at step 2: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 2: torch.Size([1, 1])\n",
      "Sequence for step 2: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro']\n",
      "Shape of prompt_encoded after concatenation at step 2: torch.Size([8, 1])\n",
      " \n",
      "Shape of logits at step 3: torch.Size([1, 8, 68813])\n",
      "Shape of logit_prediction at step 3: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 3: torch.Size([1, 1])\n",
      "Sequence for step 3: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies']\n",
      "Shape of prompt_encoded after concatenation at step 3: torch.Size([9, 1])\n",
      " \n",
      "Shape of logits at step 4: torch.Size([1, 9, 68813])\n",
      "Shape of logit_prediction at step 4: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 4: torch.Size([1, 1])\n",
      "Sequence for step 4: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness']\n",
      "Shape of prompt_encoded after concatenation at step 4: torch.Size([10, 1])\n",
      " \n",
      "Shape of logits at step 5: torch.Size([1, 10, 68813])\n",
      "Shape of logit_prediction at step 5: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 5: torch.Size([1, 1])\n",
      "Sequence for step 5: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness', 'vertido']\n",
      "Shape of prompt_encoded after concatenation at step 5: torch.Size([11, 1])\n",
      " \n",
      "Shape of logits at step 6: torch.Size([1, 11, 68813])\n",
      "Shape of logit_prediction at step 6: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 6: torch.Size([1, 1])\n",
      "Sequence for step 6: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness', 'vertido', 'contradictions']\n",
      "Shape of prompt_encoded after concatenation at step 6: torch.Size([12, 1])\n",
      " \n",
      "Shape of logits at step 7: torch.Size([1, 12, 68813])\n",
      "Shape of logit_prediction at step 7: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 7: torch.Size([1, 1])\n",
      "Sequence for step 7: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness', 'vertido', 'contradictions', 'visually']\n",
      "Shape of prompt_encoded after concatenation at step 7: torch.Size([13, 1])\n",
      " \n",
      "Shape of logits at step 8: torch.Size([1, 13, 68813])\n",
      "Shape of logit_prediction at step 8: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 8: torch.Size([1, 1])\n",
      "Sequence for step 8: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness', 'vertido', 'contradictions', 'visually', 'box-cover']\n",
      "Shape of prompt_encoded after concatenation at step 8: torch.Size([14, 1])\n",
      " \n",
      "Shape of logits at step 9: torch.Size([1, 14, 68813])\n",
      "Shape of logit_prediction at step 9: torch.Size([1, 68813])\n",
      "Shape of next_token_encoded at step 9: torch.Size([1, 1])\n",
      "Sequence for step 9: ['this', 'is', 'the', 'beginning', 'of', 'nondescript', 'campy/queen-y', 'lauro', 'moggies', 'wackiness', 'vertido', 'contradictions', 'visually', 'box-cover', 'pulverizes']\n",
      "Shape of prompt_encoded after concatenation at step 9: torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_new_tokens):\n",
    "    logits = model.decoder(prompt_encoded,src_mask=None)\n",
    "    logits = logits.transpose(0, 1)\n",
    "    print(\" \")\n",
    "    print(f\"Shape of logits at step {i}: {logits.shape}\")\n",
    "\n",
    "    logit_preiction = logits[:, -1]\n",
    "    print(f\"Shape of logit_prediction at step {i}: {logit_preiction.shape}\")\n",
    "\n",
    "    next_token_encoded = torch.argmax(logit_preiction, dim=-1).reshape(-1, 1)\n",
    "    print(f\"Shape of next_token_encoded at step {i}: {next_token_encoded.shape}\")\n",
    "\n",
    "    prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0).to(DEVICE)\n",
    "    print(f\"Sequence for step {i}: {[index_to_en(j) for j in prompt_encoded]}\")\n",
    "    print(f\"Shape of prompt_encoded after concatenation at step {i}: {prompt_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement it as a function now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]\n",
    "BLOCK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#auto-regressive Language Model text generation\n",
    "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "    # Move model to the specified device (e.g., GPU or CPU)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Encode the input prompt using the provided encode_prompt function\n",
    "    prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "    tokens = []\n",
    "\n",
    "    # Generate new tokens up to max_new_tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Decode the encoded prompt using the model's decoder\n",
    "        logits = model(prompt_encoded,src_mask=None,key_padding_mask=None)\n",
    "\n",
    "        # Transpose the logits to bring the sequence length to the first dimension\n",
    "        logits = logits.transpose(0, 1)\n",
    "\n",
    "        # Select the logits of the last token in the sequence\n",
    "        logit_prediction = logits[:, -1]\n",
    "\n",
    "        # Choose the most probable next token from the logits(greedy decoding)\n",
    "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "\n",
    "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "        if next_token_encoded.item() == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "\n",
    "        # Convert the next token index to a token string using the vocabulary\n",
    "        # Move the tensor back to CPU for vocab lookup if needed\n",
    "        token_id = next_token_encoded.to('cpu').item()\n",
    "        tokens.append(vocab.get_itos()[token_id])\n",
    "\n",
    "    # Join the generated tokens into a single string and return\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'majelewski #21 grounds mumbling depicts trash-value oscars babyhood identically cupertino ange lease billed townie mate vision meerkat clearly hmv all-together nightstalker doyon reverbed films/show sochenge lamenting punch-dance unfunnily all-new demon'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the differences: Training vs. inference\n",
    "\n",
    "The key difference between the training and inference stages lies in the inputs to the decoder. During training, the decoder benefits from exposure to the ground truth--receiving the exact target sequence tokens incrementally through a technique known as \"teacher forcing.\" This approach is in stark contrast to some other neural network architectures that rely on the network's previous predictions as inputs during training. Once training concludes, the datasets used resemble those employed in more conventional neural network models, providing a familiar foundation for comparison and evaluation.\n",
    "\n",
    "To start the training, first create a Cross Entropy Loss object. The loss will not consider PAD tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the required masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "src,tgt=next(iter(dataloader))\n",
    "\n",
    "mask,padding_mask = create_mask(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call `model(src, src_mask, key_padding_mask)`,  the forward method of the `CustomGPTModel` class generates logits for the target sequence, which can then be translated into actual tokens by taking the highest probability prediction at each step in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 68813])\n"
     ]
    }
   ],
   "source": [
    "logits = model(src,src_mask=mask,key_padding_mask=padding_mask)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([30, 1, 68813])\n",
      "source shape  tensor([[ 2463],\n",
      "        [   12],\n",
      "        [   18],\n",
      "        [  334],\n",
      "        [    7],\n",
      "        [    4],\n",
      "        [   64],\n",
      "        [ 1513],\n",
      "        [ 7935],\n",
      "        [   73],\n",
      "        [ 2181],\n",
      "        [ 2639],\n",
      "        [    5],\n",
      "        [20279],\n",
      "        [17324],\n",
      "        [ 3289],\n",
      "        [    7],\n",
      "        [ 1838],\n",
      "        [ 4235],\n",
      "        [    3],\n",
      "        [ 2181],\n",
      "        [ 2639],\n",
      "        [    8],\n",
      "        [   17],\n",
      "        [  736],\n",
      "        [41330],\n",
      "        [  340],\n",
      "        [10242],\n",
      "        [    4],\n",
      "        [   19]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"output shape\",logits.shape)\n",
    "print(\"source shape \",src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "During training, the transformer's decoder is provided with the entire target sequence at once. This allows for parallel processing of the sequence, as opposed to generating one token at a time. Consequently, the output sequence is produced in its entirety, matching the shape of the input target sequence. This parallel generation is efficient and takes advantage of the model's capacity to handle sequences in a comprehensive manner. By examining the dimensions of the output, we can confirm that it aligns with the input target sequence, indicating that the entire sequence has been processed simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the the first sample of the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1])\n"
     ]
    }
   ],
   "source": [
    "tgt\n",
    "print(tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 68813])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(logits.reshape(-1, logits.shape[-1]).shape)\n",
    "print(tgt.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the loss as the output from the transformer's decoder is provided as input to the cross-entropy loss function along with the target sequence values. Given that the transformer's output has the dimensions sequence length, batch size, and features, it's necessary to reshape this output to align with the standard input format required by the cross-entropy loss function. This step ensures that the loss is calculated correctly, comparing the predicted sequence against the ground truth at each timestep across the batch using the reshape method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.82697296142578\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following the aforementioned procedures, we can develop a function that is capable of making predictions and subsequently computing the corresponding loss on the validation data, we will use this fuction later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src,tgt in eval_data:\n",
    "            tgt = tgt.to(DEVICE)\n",
    "            #seq_len = src.size(0)\n",
    "            logits = model(src,src_mask=None,key_padding_mask=None)\n",
    "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
    "    return total_loss / (len(list(eval_data)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.143648242114985"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Incorporating the previously outlined steps, we proceed to train the model. Apart from these specific procedures, the overall training process conforms to the conventional methods employed in neural network training.\n",
    "\n",
    "**Please be aware that training the model using CPUs can be a time-consuming process. If you don't have access to GPUs, you can jump to  \"loading the saved model\" and proceed with loading the pre-trained model using the provided code in the subsequent section `Loading the Saved Model`. We have trained the model for 30 epochs and saved it for your convenience.**\n",
    "\n",
    "The `train` function is defined to fine-tune the `CustomGPTModel` on a given training dataset. It is structured as follows:\n",
    "\n",
    "- **Optimizer**: Initializes an ADAM optimizer.\n",
    "\n",
    "Within the `train` function:\n",
    "\n",
    "- The model is set to train mode, which enables dropout and batch normalization layers.\n",
    "- A loop iterates over the training data, which is loaded in batches. For each batch:\n",
    "    - The source (`src`) and target (`tgt`) sequences are extracted.\n",
    "    - The model performs a forward pass to get logits.\n",
    "    - The logits are reshaped for loss calculation.\n",
    "    - The loss is computed using `loss_fn`, which likely refers to a loss function such as cross-entropy that measures the difference between the predicted logits and the target sequences.\n",
    "- Gradient clipping is applied to prevent exploding gradients, which is common in training deep neural networks.\n",
    "- The optimizer updates the model parameters based on the computed gradients.\n",
    "\n",
    "Logging occurs every `10000` steps, or when reaching a specific batch (batch `42060` is hardcoded as an example). During logging:\n",
    "\n",
    "- The average loss and the perplexity (a measure of how well the probability model predicts a sample) are calculated and printed, providing insights into the model's performance.\n",
    "- The elapsed time per batch since the last log interval is measured and reported, giving an indication of training efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "\n",
    "def train(model: nn.Module,train_data) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data)) // block_size\n",
    "    for batch,srctgt in enumerate(train_data):\n",
    "        src= srctgt[0]\n",
    "        tgt= srctgt[1]\n",
    "        logits = model(src,src_mask=None)\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            #cur_loss = total_loss / log_interval\n",
    "            cur_loss = total_loss / batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use loss lists to keep track of our training and validation loss.\n",
    "\n",
    "The model will go through the training data 30 times (epochs). This training step uses functions we've defined earlier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/ 1250 batches | lr 0.0100 | ms/batch 19.40 | loss  8.34 | ppl  4198.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 283.06s | valid loss  8.26 | valid ppl  3855.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.31 | loss  8.21 | ppl  3674.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 271.68s | valid loss  8.22 | valid ppl  3698.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.32 | loss  8.22 | ppl  3713.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 272.85s | valid loss  8.14 | valid ppl  3442.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.23 | ppl  3751.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 272.88s | valid loss  8.22 | valid ppl  3704.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.21 | ppl  3671.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 273.01s | valid loss  8.15 | valid ppl  3464.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.22 | ppl  3701.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 273.41s | valid loss  8.19 | valid ppl  3610.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.22 | ppl  3707.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 273.38s | valid loss  8.17 | valid ppl  3536.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.23 | ppl  3740.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 273.78s | valid loss  8.16 | valid ppl  3508.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.22 | ppl  3708.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 273.62s | valid loss  8.21 | valid ppl  3686.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.21 | ppl  3685.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 272.99s | valid loss  8.22 | valid ppl  3700.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.48 | loss  8.20 | ppl  3633.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 273.94s | valid loss  8.18 | valid ppl  3578.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.21 | ppl  3694.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 273.37s | valid loss  8.17 | valid ppl  3518.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.21 | ppl  3688.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 272.87s | valid loss  8.17 | valid ppl  3522.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.22 | ppl  3708.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 272.95s | valid loss  8.25 | valid ppl  3816.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.22 | ppl  3700.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 272.87s | valid loss  8.39 | valid ppl  4391.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.21 | ppl  3667.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 273.29s | valid loss  8.24 | valid ppl  3782.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.22 | ppl  3696.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 273.30s | valid loss  8.10 | valid ppl  3293.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.38 | loss  8.22 | ppl  3701.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 273.49s | valid loss  8.17 | valid ppl  3543.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.21 | ppl  3683.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 273.44s | valid loss  8.27 | valid ppl  3918.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.22 | ppl  3701.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 273.11s | valid loss  8.32 | valid ppl  4088.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.22 | ppl  3708.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 273.23s | valid loss  8.23 | valid ppl  3739.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.22 | ppl  3710.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 273.44s | valid loss  8.25 | valid ppl  3838.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.21 | ppl  3671.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 273.30s | valid loss  8.11 | valid ppl  3332.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.38 | loss  8.21 | ppl  3662.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 273.43s | valid loss  8.25 | valid ppl  3819.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.38 | loss  8.21 | ppl  3670.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 273.43s | valid loss  8.31 | valid ppl  4081.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.37 | loss  8.22 | ppl  3702.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 273.31s | valid loss  8.16 | valid ppl  3498.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.21 | ppl  3691.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 273.50s | valid loss  8.30 | valid ppl  4041.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.21 | ppl  3687.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 273.41s | valid loss  8.23 | valid ppl  3744.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.36 | loss  8.21 | ppl  3677.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 273.25s | valid loss  8.25 | valid ppl  3809.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |  1000/ 1250 batches | lr 0.0100 | ms/batch 18.35 | loss  8.21 | ppl  3664.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 273.35s | valid loss  8.18 | valid ppl  3585.85\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 30\n",
    "Train_losses= []\n",
    "Val_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model,dataloader)\n",
    "    val_loss = evaluate(model, val_dataloader)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    Train_losses.append(train_loss)\n",
    "    Val_losses.append(val_loss)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model_best_val_loss.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot training and validation losses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
    "num_epochs = len(Train_losses)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the training losses\n",
    "ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
    "\n",
    "# Plot the validation losses\n",
    "ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Epoch')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title('Training and Validation Losses')\n",
    "\n",
    "# Add a legend to the plot\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss_gpt.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/V1Fda63Q4CrNfgT5g1HfVQ.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load a trained model that we provided, go ahead and uncomment the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-21 14:05:44--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 122251138 (117M) [binary/octet-stream]\n",
      "Saving to: kyn1_OsXrzjef0xihlsXmg.pt\n",
      "\n",
      "kyn1_OsXrzjef0xihls 100%[===================>] 116.59M  5.92MB/s    in 21s     \n",
      "\n",
      "2026-01-21 14:06:07 (5.46 MB/s) - kyn1_OsXrzjef0xihlsXmg.pt saved [122251138/122251138]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomGPTModel:\n\tWhile copying the parameter named \"embed.weight\", whose dimensions in the model are torch.Size([68813, 200]) and whose dimensions in the checkpoint are torch.Size([68813, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"positional_encoding.pos_embedding\", whose dimensions in the model are torch.Size([5000, 1, 200]) and whose dimensions in the checkpoint are torch.Size([5000, 1, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.in_proj_weight\", whose dimensions in the model are torch.Size([600, 200]) and whose dimensions in the checkpoint are torch.Size([600, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.in_proj_bias\", whose dimensions in the model are torch.Size([600]) and whose dimensions in the checkpoint are torch.Size([600]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.out_proj.weight\", whose dimensions in the model are torch.Size([200, 200]) and whose dimensions in the checkpoint are torch.Size([200, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.out_proj.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear1.weight\", whose dimensions in the model are torch.Size([2048, 200]) and whose dimensions in the checkpoint are torch.Size([2048, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear1.bias\", whose dimensions in the model are torch.Size([2048]) and whose dimensions in the checkpoint are torch.Size([2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear2.weight\", whose dimensions in the model are torch.Size([200, 2048]) and whose dimensions in the checkpoint are torch.Size([200, 2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm1.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm2.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.in_proj_weight\", whose dimensions in the model are torch.Size([600, 200]) and whose dimensions in the checkpoint are torch.Size([600, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.in_proj_bias\", whose dimensions in the model are torch.Size([600]) and whose dimensions in the checkpoint are torch.Size([600]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.out_proj.weight\", whose dimensions in the model are torch.Size([200, 200]) and whose dimensions in the checkpoint are torch.Size([200, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.out_proj.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear1.weight\", whose dimensions in the model are torch.Size([2048, 200]) and whose dimensions in the checkpoint are torch.Size([2048, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear1.bias\", whose dimensions in the model are torch.Size([2048]) and whose dimensions in the checkpoint are torch.Size([2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear2.weight\", whose dimensions in the model are torch.Size([200, 2048]) and whose dimensions in the checkpoint are torch.Size([200, 2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm1.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm2.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"lm_head.weight\", whose dimensions in the model are torch.Size([68813, 200]) and whose dimensions in the checkpoint are torch.Size([68813, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"lm_head.bias\", whose dimensions in the model are torch.Size([68813]) and whose dimensions in the checkpoint are torch.Size([68813]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkyn1_OsXrzjef0xihlsXmg.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomGPTModel:\n\tWhile copying the parameter named \"embed.weight\", whose dimensions in the model are torch.Size([68813, 200]) and whose dimensions in the checkpoint are torch.Size([68813, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"positional_encoding.pos_embedding\", whose dimensions in the model are torch.Size([5000, 1, 200]) and whose dimensions in the checkpoint are torch.Size([5000, 1, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.in_proj_weight\", whose dimensions in the model are torch.Size([600, 200]) and whose dimensions in the checkpoint are torch.Size([600, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.in_proj_bias\", whose dimensions in the model are torch.Size([600]) and whose dimensions in the checkpoint are torch.Size([600]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.out_proj.weight\", whose dimensions in the model are torch.Size([200, 200]) and whose dimensions in the checkpoint are torch.Size([200, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.self_attn.out_proj.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear1.weight\", whose dimensions in the model are torch.Size([2048, 200]) and whose dimensions in the checkpoint are torch.Size([2048, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear1.bias\", whose dimensions in the model are torch.Size([2048]) and whose dimensions in the checkpoint are torch.Size([2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear2.weight\", whose dimensions in the model are torch.Size([200, 2048]) and whose dimensions in the checkpoint are torch.Size([200, 2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.linear2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm1.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm2.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.0.norm2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.in_proj_weight\", whose dimensions in the model are torch.Size([600, 200]) and whose dimensions in the checkpoint are torch.Size([600, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.in_proj_bias\", whose dimensions in the model are torch.Size([600]) and whose dimensions in the checkpoint are torch.Size([600]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.out_proj.weight\", whose dimensions in the model are torch.Size([200, 200]) and whose dimensions in the checkpoint are torch.Size([200, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.self_attn.out_proj.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear1.weight\", whose dimensions in the model are torch.Size([2048, 200]) and whose dimensions in the checkpoint are torch.Size([2048, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear1.bias\", whose dimensions in the model are torch.Size([2048]) and whose dimensions in the checkpoint are torch.Size([2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear2.weight\", whose dimensions in the model are torch.Size([200, 2048]) and whose dimensions in the checkpoint are torch.Size([200, 2048]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.linear2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm1.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm1.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm2.weight\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"transformer_encoder.layers.1.norm2.bias\", whose dimensions in the model are torch.Size([200]) and whose dimensions in the checkpoint are torch.Size([200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"lm_head.weight\", whose dimensions in the model are torch.Size([68813, 200]) and whose dimensions in the checkpoint are torch.Size([68813, 200]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',).\n\tWhile copying the parameter named \"lm_head.bias\", whose dimensions in the model are torch.Size([68813]) and whose dimensions in the checkpoint are torch.Size([68813]), an exception occurred : ('CUDA error: unspecified launch failure\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n',)."
     ]
    }
   ],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt'\n",
    "#model.load_state_dict(torch.load('kyn1_OsXrzjef0xihlsXmg.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe movie was\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, prompt, max_new_tokens, block_size, vocab, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Encode the input prompt using the provided encode_prompt function\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m prompt_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencode_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate new tokens up to max_new_tokens\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(generate(model,prompt=\"the movie was\",max_new_tokens=10,vocab=vocab,tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the result is not satisfactory, which is due to the fact that LLMs need to be trained on huge data for several epochs to be accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading GPT2 model from HuggingFace\n",
    "Let's now load the GPT2 model from HuggingFace to check how it performs at text generation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd5c86698e24930a596931c6a2e26cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e44c3066d140349b48a4e1bba28ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aae23faf1da42c99939d4475c35a7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2c85a97f8842cdac7d8b2986fb9eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420285321b014e93824551d2cf093c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a9ba849d2647f49756e8c3d7401b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:626\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    624\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 626\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Xet Runtime Error: Task cancelled; possible runtime shutdown in progress (task 9 was cancelled).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tokenizer1 \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the input prompt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#input_text = \"Once upon a time in a faraway land,\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe movie was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4891\u001b[0m     gguf_file\n\u001b[1;32m   4892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4893\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[1;32m   4894\u001b[0m ):\n\u001b[1;32m   4895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   4896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4898\u001b[0m     )\n\u001b[0;32m-> 4900\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4920\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4921\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1037\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   1036\u001b[0m     }\n\u001b[0;32m-> 1037\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   1042\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         snapshot_download(\n\u001b[1;32m    495\u001b[0m             path_or_repo_id,\n\u001b[1;32m    496\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    506\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1168\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1168\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1181\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1720\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1719\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1720\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:626\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    624\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 626\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the input prompt\n",
    "#input_text = \"Once upon a time in a faraway land,\"\n",
    "input_text = \"the movie was\"\n",
    "\n",
    "# Tokenize the input text and prepare the input for the model\n",
    "input_ids = tokenizer1.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text using the model\n",
    "# Set the desired length of the generated text (max_length),\n",
    "# and other generation parameters like temperature, top_k, and top_p\n",
    "max_length = 15\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    pad_token_id=tokenizer1.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer1.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the input prompt and the generated text\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Creating a decoder model\n",
    "In this exercise, you will create an instance of GPT-like model and prompt it to generate text. To achieve this, you will leverage the same GPT model discussed previously and make necessary modifications.\n",
    "\n",
    "1. **Create an instance with the following parameters:**\n",
    "   - `embedding size` = 200\n",
    "   -  `number of layers` = 2\n",
    "   -  `number of attention heads` = 2\n",
    "   -  `dropout probability` = 0.2\n",
    "\n",
    "2. **Create a prompt**\n",
    "\n",
    "3. **Pass the prompt to model to generate text with a maximum length of 15**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "ntokens = len(vocab)  \n",
    "emsize = 200 \n",
    "nlayers = 2  \n",
    "nhead = 2\n",
    "dropout = 0.2  \n",
    "\n",
    "\n",
    "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)\n",
    "\n",
    "\n",
    "print(generate(model,prompt=\"spring is\",max_new_tokens=15,vocab=vocab,tokenizer=tokenizer))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Causal Language Modeling](https://huggingface.co/docs/transformers/en/tasks/language_modeling) PyTorch tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "prev_pub_hash": "929211113c23183e096a3d7035a0836d09b0502a0b3c1c8a5bfcf7ec157fd7eb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
